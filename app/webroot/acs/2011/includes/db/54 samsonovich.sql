insert into review values (13, 54, 'related', 'Highly related', "The topic of this paper is very relevant to high-level cognition almost by definition. It promises to examine issues in problem solving and metacognition involved in self-regulated learning. The paper integrates an implemented cognitive architecture and data from actual human subjects.");
insert into review values (13, 54, 'extension', 'Insubstantial extension', "It is very difficult to evaluate the capabilities of the cognitive system discussed in this paper. The authors briefly describe an implemented portion of an architecture called CC, but the details are not provided. Some pointers to other publications are provided instead. Most of this paper concentrates upon a human experiment performed in the laboratory to collect data. It is also not clear the relationship between CC and GMU BICA.");
insert into review values (13, 54, 'claims', 'Vague claims', "The claims in this paper are very confusing to the reader. The claims come in two sets. First they make broad foundational claims and second they make more narrow methodological claims. They examine high-level cognition and metacognition in the form of self-regulated learning. They claim that to make progress in the understanding of this phenomena, one requires a precise theory. The preferred theory for the authors take the form of a cognitive architecture (although they do not state how the architecture itself represents or supports the theory). To validate the theory one needs a solid measurement tool or data collection method that enables detailed analysis. A validated cognitive theory then will enable sound applications such as automated assistants in self-regulated learning tasks. The problem with the claims lay in the execution and in particular with the choice of data collection tool. 
Methodological claims seem to state that they have used their cognitive architecture as a data collection tool that not only records human responses during cognitive tasks but that provides a direct or indirect recording of the cognition and metacognition that drives the human responses. This is a flawed assumption, because one can only record overt behavior. There is no 'window into the student's mind at the metacognitive level (p. 2)' that is open for inspection. We simply make inferences concerning the psychological world given human behavior and reasonable assumptions. The authors' assumptions are not reasonable. 
Equally important, however, is the apparent contradiction that results from taking these two sets of claims in conjunction. The first states that the architecture represents a theory and that validation of the theory depends upon a sound tool. The second states that the architecture constitutes the data collection tool. I do not understand how the architecture can be both the theory and the tool that validates the theory.");
insert into review values (13, 54, 'convincing', 'Not convincing', "This evidence in this paper is very hard to understand and to evaluate. Many of the effects are statistically insignificant and thus independent. This independence is then considered as evidence for allowing the use of the data collection method in a manner that does not affect performance of the subjects. However the performance task is not explained very well. One example is provided in the Study Material section, but the authors do not use this example to explain how the procedure works. The paper would benefit from an explanation that relates the example problem to the kinds of diagrams constructed in Figure 3. This explanation could also be related to the kinds of 'motives' shown in Figure 4. The example in the discussion ([a] through [e] on page 7) might be useful for explaining to the reader the exact nature of the subjects forethought tasks. 
I also question the experimental design such that 41 participants were placed into 32 groups.");
insert into review values (13, 54, 'effective', 'Not effective', "This is an extremely poorly written paper. The problem has in part to do with the use of the English language; but more importantly the paper is mismanaged in its logical structure and organization. Clarity is absent.");
insert into review values (13, 54, 'comment', '', "The introduction starts the paper on the wrong foot. It is poorly written and does not explain the motivation, main results or the impact of this research in a compelling fashion. It begins with an assertion that a lack of understanding exists with respect to human mathematical problem-solving. However I think that a large body of results exists in this area. The authors should examine John Anderson's and Ken Koedinger's work on math tutors and the ACT-R architecture at CMU (see http://ctat.pact.cs.cmu.edu/index.php?id=timeline). You might also look at VanLehn's early research  (VanLehn, 1982; 1986). Also the authors never related Figure 1 to the remainder of the paper in any detail. 
Another problem with the paper's methodology is that untrained human subjects will not understand cognitive technical constructs such as 'intermediate goals' or other terms used in their forethought task. This almost seems to constitute a kind of nineteenth century introspectionism as advanced by Titchener (please see Boring, 1953). 

Boring, E. G. (1953). A history of introspection. Psychological Bulletin, 50(3), 169-189

VanLehn, K. (1986). Arithmetic procedures are induced from examples. In J. Hiebert (Ed.), Conceptual and Procedural Knowledge: The Case of Mathematics (pp. 133-180). Erlbaum, Hillsdale, NJ.

VanLehn, K. (1982). Bugs are not enough: An analysis of systematic subtraction errors. Journal of Mathematical Behavior, 3(2), 3-71.");
insert into review values (13, 54, 'meeting', 'Reject paper', "");
insert into review values (13, 54, 'journal', 'Reject paper', "");